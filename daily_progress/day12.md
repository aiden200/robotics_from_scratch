### **Day 12**

- **PAPER: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a STochastic Actor Pt 1**
  - Struggles with model free deep RL
  - Entropy maximization term
  - Maximum Entropy Reinforcement Learning
  - Soft policy iteration
- **Paper Implementation: PPO pt 5 / Omniverse Development pt 1**
  - Started configuring the cartpole environment NVIDIA Omniverse
  - Managed to get a cube in Omniverse, display object info (Velocity, Position) every simulated step.
  - PPO Code located in [here](../code/models/ppo.py)
  - cartpole environment located in [here](../code/environment/cartPole.py)
  - Things left: Environment configuration, Actor Parallel, minibatch updates

### **Notes**

<div style="display: flex; justify-content: space-between;">
  <img src="../assets/day_12_paper_cover.jpg" alt="Paper notes 1" width="45%">
  <img src="../assets/day_12_paper_1.jpg" alt="Paper notes 1" width="45%">
</div>
<br>
<div style="display: flex; justify-content: space-between;">
  <img src="../assets/day_12_paper_2.jpg" alt="Paper notes 1" width="45%">
  <img src="../assets/day_12_omniverse_1.png" alt="Paper notes 1" width="45%">
</div>
